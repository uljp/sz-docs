## 前言 Preface

一直希望找个机会写写关于数字空间相关的事情，正好趁最近优锘小学组织的划未来活动在火热进行中，我就报名了。

很多同事可能并没有直接感知到数字空间提供了哪些服务，其实个问题其实我们也意识到了，数字空间写了一堆服务，可能有一些服务大家都熟悉的，比如[优锘官网](https://www.uino.com)以及[ThingJS-X的在线环境](https://www.thingjs.com/thingjs-x/digitalSpace/home)，目前每天运行的在线环境总数在700个左右。对于这些领域相关也逐渐形成了一些相对最佳实践，所以写下此文，希望有机会还是尽量形成标准能力对外输出。年初，孔峰找我组织数字办部门，首次提出了数字空间的概念，当时还是比较模糊的，目的就是要把日常线下工作尽可能的搬迁到线上。

我听完后预判这可能是一堆子系统组成的一个非常复杂的业务服务系统，

![架构图](https://pan.udolphin.com/files/image/2021/10/2454dc7cbe6c0d068508e0c66d7f977b.png)

那这些一堆子系统如何归类，编排，配置，依赖衔接，对外提供稳定可靠服务是一个巨大挑战，这中间涉及到本地开发环境配置、预发布前测试、CICD集群部署、上百个服务数据备份、海量日志抓取、健康监测、故障排查、熔断扩容、服务恢复、高并发等一系列挑战。

下面是数字空间目前采用了哪些技术与实践手段来保证这些服务的快速部署、与系统可靠性。


## 微服务 Microservices

![ms2.png](https://pan.udolphin.com/files/image/2021/10/7ca594c8fef6ab28c2a726c3abb11634.png)

把一个庞大的系统，拆分成多个微型的服务，是绝大多数SaaS平台的最佳实践，我们自然也这么干，但这里难的不是要不要拆，而是怎么拆？拆到多细？这需要根据业务场景以及经验去做尝试，直到找到一个相对感觉平衡的颗粒度即可。比如在数字空间里的微服务分类，大概分成以下几类

- `平台服务`：Kafka，Kubernetes，Service Mesh，Logs，共享存储，CDN等
- `关键服务`：鉴权网关，消息网关（手机短信、邮箱、企业微信等），接入层API网关，流量统计网关
- `业务服务`：优锘官网，U知，评论系统，人机验证码校验，CICD系统，Git服务等等

针对不同类型的服务，我们所采取针对的配置，监控级别将不一样。如平台服务或者关键服务出问题影响范围都是很大的，而业务类服务则就放宽很多，但实际上也有业务类服务需要高度保障的，比如类似优锘官网这种，假如访问不了或者打开速度缓慢将会特别影响公司的业务推广。

那我们是如何拆分一个服务的？以[U知](https://wiki.uino.com)服务来举例子

### U知微服务的拆分

####  用户授权服务

用户授权服务本身业务就很复杂，涉及到角色，权限，登录方式，账号启停等等一些列问题，很多服务也需要用户系统授权支撑，很自然的就把这一块交给数字空间SSO，经过数字空间SSO统一认证登录后的用户等于进了领域大门，具体业务强耦合细颗粒度的控制就可以放到U知去做自然就省事很多。

#### 文章图片

U知文章中夹带的图片，如果不把这块服务拆分的话U知就要开发相应的模块，支持用户上传各类文件，允许用户上传各类文件实际上是一个比较麻烦的业务，比如上传文件限制大小，文件类型，上传后如何管理等等。

考虑到可能其它服务也会使用到此类服务，因此我们决定单独把这块业务拉出来，对接我们之前写好的一个[小墙](https://pan.udolphin.com)服务，所有文件大小检测，类型判断，上传带宽，存储归档，外部访问都由小墙这个服务来提供，目前小墙已经做为数字空间的文件存储基础服务，方便上层各类应用系统对接，每天有大量的文件上传与读取。

![小墙](https://pan.udolphin.com/files/image/2021/10/ea24965918f0861a371334b91b87f4fd.png)

#### 文章评论

用户看完一篇文章之后，想发表一下评论，写写个人观点，增加互动性，这个看似普通的功能其实非常有讲究，实际上业务是相对很复杂的。评论里可能会带图片，还会夹带代码块，还有少量的格式化。我们考虑到别的业务系统也需要，于是单独也把评论系统拿出来做成了一个基础服务。所以目前U知上大家看到的评论实际上是一个独立的服务，如果你们仔细观察的话，也会发现数字空间的评论UI长的都一样，那就是因为这都是由评论系统提供的能力。而它们之间也是互相引用的关系，比如评论里用户上传的图片对接的就是上面提到的小墙服务。

![评论系统](https://pan.udolphin.com/files/image/2021/10/bae1b8338a3c124b0c10b1ca78703e75.png)

#### 人机验证

最后就是人机验证，目前使用人机校验来组织机器人提交数据的攻击已是流行的一种手段，在U知里可能有些表单提交比较敏感，为了避免被Bot扫描攻击，实际上人机验证是一个普遍性的需求，如果在U知里把这块功能给做了很难共享给其它应用使用，于是我们决定把人机校验单独拿出来做成一个公共基础服务。就类似Google的recaptcha，使用它可以轻松的在自己的站点里接入人机验证服务，目前数字空间的人机验证也在对外开放，有需要的同学可以注册试用[人机验证服务](https://vcode-controller.uino.cn/)，有问题随时反馈。

![人机校验](https://pan.udolphin.com/files/image/2021/10/f226fbc70e527261ee891582ac2c54eb.png)


## 单点故障 SPOF

![spof.webp](https://pan.udolphin.com/files/image/2021/10/277944201943701a556b3c57354d14fb.webp)

程序即使写的再好也难免会有疏忽，所以有Bug是永远避免不了的，我们需要做的就是如何降低因为程序故障带来的影响，如果存在单点故障，那服务可用性降为零，那都是非常致命的，尤其是一些关键性业务。所以数字空间在系统开发，架构部署初期就已考虑过单点故障的问题，那么数字空间是如何解决单点故障的呢？

主要从以下几方面

### 多副本 Replicas

![rs.jpg](https://pan.udolphin.com/files/image/2021/11/bc4b880d68eddb0491af0953b0ad9861.jpg)

每一个服务类程序（就是进程永远不退出，比如Nginx，有些程序是Job类，完成后就退出进程）至少启动两个主进程以上，并且这两个进程是在不同的节点（此处的节点可能是虚机，也可能是物理机）上，在它们之上必须有个LB负载

数据库类也都是部署为集群模式，数字空间主要使用两类数据MongoDB，Postgres，它们根据不同的业务场景搭建不同的集群模式，目的就是满足业务需求，多副本，任意时刻某个节点挂了不影响系统运行服务。

### 无状态 Stateless App

![stateless.jpeg](https://pan.udolphin.com/files/image/2021/10/0144db76863d5ba84e101294f9aaef26.jpeg)

因为每个程序都部署多个副本，前面有4层Load Balance，那么就必须做到应用程序都是无状态的，即每一个请求无论落到哪台节点的应用进程，理论上得到的响应数据应该都是无差别的。

但是，很多场景下应用程序都是需要有状态的，此时解决方案就是把这些有状态的数据集中管理起来，比如搭建一个多节点Redis集群，然后把这些状态写Redis里，实现应用无状态，副本可以随着压力水平扩容。

### 负载均衡 Load Balance

![lb.png](https://pan.udolphin.com/files/image/2021/10/dfeed6e8a92c1fc18b08987880cc272a.png)

域名解析上，我们做了多条A记录，避免单一入口IP的服务挂掉或者被封掉的影响。另外一种也可以用云商提供的LB，这种情况就在DNS做CNAME，最终得到可靠的LB的IP地址，由云商的LB服务做负载均衡，健康检查等等。

### CDN

![CDN_834X550.png](https://pan.udolphin.com/files/image/2021/11/8ec4e3d21e43d7886e820f133e3604c6.png)

对于大部分普通的静态文件，我们尽量都部署利用CDN来分流，这样不仅带来本地用户访问速度的提升，还能大量减少无关紧要的请求到达VPC，减轻不少压力。使用CDN不复杂，但如何把它融入到自己的环境中，在部署时自动分发是需要做些前置工作的，因为有些应用每天发布更新十多次，这个过程都是自动化过程，无需干预，其中就包括CDN分发。


## 水平扩容 Horizontal Scaling

![horizontal.png](https://pan.udolphin.com/files/image/2021/10/c4130e16471ba64db7eaa64a4a1ce614.png)

有些关键性服务，比如SSO处理的请求量就有点大，截止目前统计最高峰每秒10w+请求，此时就可以根据一定的策略，如：

- 响应时间超过多少ms
- 5XX状态码次数
- 一段时间的请求量增长趋势

当达到以上列出来的某项阀值时，自动扩容多个副本，因为都是无状态应用，所以扩容副本非常容易，配置也无需变更。一段时间之后，请求量下降，就缩减副本，降低资源占用。

## 快速部署 Fast-Deploy

数字空间全部服务都是使用容器化部署，容器化技术一律采用Docker，要实现快速部署就要做到以下几点

### 镜像体积小 Small-Image

![Screen Shot 2021-10-26 at 7.40.58 PM.png](https://pan.udolphin.com/files/image/2021/10/5cab342eb9b2cd00382c5dfd931b1cd8.png)

咱们公司主要的技术栈是Java，人数众多，在做数字化技术选型的时候，也考虑过Java。有这么一句老话：`当不知道用什么技术时，选择Java总不会错。`

但在数字空间这个场景里，Java首先就被排除掉，因为在这个场景下不涉及跨平台，干同样的事情因为JDK要多占用很多空间，会把镜像体积搞得很臃肿，影响集群分发部署的效率。目标平台的性能上性能上也要优于脚本脚本语言，这个在容器化是非常凸显优势

但数字空间团队是由一批年轻的团队组成，大部分团队成员的经验，综合能力远没有到达以上描述的要求，所以部分非关键行的服务也会使用脚本语言NodeJS(Deno)编写，但即使这样，在选择母镜像时也都尽量选择`alphine`类型的。

###  抽象公共层 Base-Layer

![docker-layers.png](https://pan.udolphin.com/files/image/2021/10/c67c7807a790897aa3ebb80c2594985a.png)

我们知道镜像是分层的，Docker给每一个层都计算出一个唯一的Hash值，在节点上docker pull拉取一个镜像时，会先得到一堆Hash组成的数组，每一个Hash值表示一层，Docker会判断本地是否存在相同的Hash层，存在就直接复用，最终只拉取本地缺少的Hash层。这个设计有效的提升了镜像复用，快速拉取的效率。

所以在构建镜像时，我们就注意按分层构建镜像，这样在大规模部署时可以更快的拉取所需的镜像。

### 自动化部署 Auto-Deploy

![Screen Shot 2021-10-26 at 7.42.21 PM.png](https://pan.udolphin.com/files/image/2021/10/7b089bcae5ee48cbcb5fe2ac1b14cec3.png)

数字化目前所有服务都使用CICD自动化部署方式部署服务方式，由于采用无人干预，每天可以低成本的部署多次，开发人员只要`push tag`，会走以下流程：

- 安装依赖
- 编译
- 构建Docker镜像
- 推送到Registry
- 滚动部署

中间可能还会有一些不一样的步骤，这取决于具体项目，假如这个过程中有任何一步失败，将会停止往下走，并会抓取当前失败日志给推送者发送邮件。CICD自动化部署的方式使得我们每次发布更新都更加稳定可靠，不会出现`我电脑上看起来是好的，怎么部署就有Bug`的问题。


## 集群模式 Cluster-DB

![pg_cluster.jpeg](https://pan.udolphin.com/files/image/2021/10/4d747ac1fba0b4de82d88836a6af0a95.jpeg)

即使应用程序都写成无状态，但数据库如果是单节点模式还是容易形成瓶颈以及存在单点故障的风险，为了消除这个瓶颈在数字空间里所有的数据库都部署成集群模式。如前面说过，数字空间主要使用两类数据库，第一个是MongoDB，第二个是Postgres，这两个数据库官方都提供了多场景的集群模式。

而另外一些Redis，Zookeeper，Kafka也都是集群模式部署，集群模式的缺点就是CAP理论只能满足其中两个，因此我们必须要根据业务场景做取舍，没有一种模式是通杀所有场景的，只要能满足业务需求就可以。

## 每日备份 Backup

![data-backup.png](https://pan.udolphin.com/files/image/2021/10/bafe6c7998ef151ebc686c7e78368259.png)

对于数字空间来说数据永远是最重要的，目前数字空间每天固定跑一次备份，实践方式就是按年月日建目录，然后再按应用建目录，挨个使用DB提供的导出命令进行备份。比如Gitlab每天的备份文件大小超过50G。

这么多的备份文件没有一个大的磁盘是不够的，好在数字空间架构本质上就是云原生架构，我们充分利用了云优势，把备份文件安全的存储起来，并且对备份文件做加密。

## 安全 Security

![security.jpeg](https://pan.udolphin.com/files/image/2021/10/9934a967b2fa71e84859e5df8251342d.jpeg)

数据怎么保障安全呢？根据数字空间目前的业务来看，重点从以下几方面入手

### 生产测试严格分离

我们在公司的本地的VPC里建了一套几乎与生产生产一模一样的环境，遇到不好解决定位的问题，我们都会在内网的环境里复现修改再测试，确认无误后再部署到生产。就连SSO我们也都部署了一套在测试环境，虽然也额外增加了一部分工作量，但某些问题的场景下非常有必要。

在测试环境，我们会固定部署分支程序，比如master分支，而在生产环境，我们严格要求必须要有版本管理，目前通常都是使用大家都通用的`Semantic Versioning 2.0.0`，它由3段组成，分别是

- `Major`: 主版本号，通常是架构发生重大变更
- `Minor`: 中间版本号，通常增加新功能
- `Patch`：补丁版本号，通常用于Bug修复或者其它非功能修改

`git push tag`之后会在生产部署对应版本的代码。这么做是为了发生问题时可以会退上个版本，并且可以对有问题的版本代码进行排查，修复。

### 强制TLS

![tls.png](https://pan.udolphin.com/files/image/2021/10/bb4c3472498fa067b0ba11587b71f24d.png)

数字空间提供的所有Web服务，在用户终端到VPC网关这一段走的是互联网，互联网的环境异常复杂，为了安全一律强制在这一段使用HTTPS模式，避免HTTP报文被嗅探器捕获从而提取到传输中的用户数据。

而为了性能，在VPC内数据包的传递就不强制走SSL模式，比如边际网关到应用之间的数据包全都走普通模式，所以VPC就不能有鱼龙混杂的程序在里头跑，它是一片净土。

## 性能 Performance

如何提高数字空间提供服务的请求处理并发吞吐量，是一项非常重要并且复杂的工作。这得根据不同的业务场景找出瓶颈点，然后解决它！

### 技术选型

以优锘官网为例子，优锘官网有以下特性

- 大部分都是静态页面
- 无会话
- 图片文字多
- 几乎没表单
- 动态内容少
- 页面响应速度要快
- SEO友好支持

对于以上几个特性，在技术选型上就贴合以上特性做选择

- 不使用SPA框架，因为SPA应用不仅首页响应慢，SEO也不友好
- 内容走CDN分发，物理上约接近用户的位置理论上响应速度越快
- 多域名分流资源，浏览器有同源并发限制，但一个页面可能附带很多附属资源，比如图片，字体，脚本，样式等等，我们单独为每一类资源都走不同的域名，解决同源并发限制的问题
- 服务器端渲染，因为都是静态页面，所以我们在服务器端提前把HTML页面渲染好，然后再响应给浏览器端
- 服务器端压缩内容，尽可能的把要传输的文件，文本都压缩一下，可以极大节省传输时延，有些文本文件的压缩比甚至高达百分之九十以上，非常可观
- 其它技术优化，比如脚本延迟加载，图片懒加载等等

经过以上的优化，可以极大提升官网的打开速度，这些优化手段没有固定模式，取决于业务本身，找到瓶颈并解决它。

### 缓存 Cache

在计算机里一切性能问题都可以通过缓存解决，我们尽可能的去缓存，我们尽可能的把那些访问频繁的数据做一层缓存，这的确是有效增加了IO，但增加缓存就会容易带来数据一致性的问题。这个具体需求具体调节，实际上大部分场景都不是要求数据实时的，这也没有实际意义。

打个比方，比如统计在线人数，其实23798，和23.9K都是可以的，实际上这个数据是有点延迟的，但是要为了实时性每个用户访问页面都要实时计算在线人数所消耗的资源对这个业务场景并没有什么意义，所以类似这样的地方，我们都是每隔一段时间算好缓存，极大提升响应速度。

## 最后

对于目前来说仍然存在许多挑战，我们也在根据自己的业务继续寻找最佳实践，同时数字空间也大量缺人手，尤其是用户验证体系，过去它只服务于数字空间的服务，未来它要服务全公司的SaaS类服务，即将迎来更严峻的挑战，长期招兵买马。

希望数字空间提供的服务稳定可靠，让大家用的放心。

我是数字办的忠哥，目前也同期在录制[《忠哥网络知识分享》](https://learn.uino.com/kng/#/course/detail?id=&courseId=2e93d83f-2718-429a-9c35-590f0e25b39b)并且发布在优锘小学上，希望大家多多支持！

